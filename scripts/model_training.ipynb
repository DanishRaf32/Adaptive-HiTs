{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ResNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Danish Rafiq and Asif Hamid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code originally created by Yuying Liu,\n",
    "This script is a template for training neural network time-steppers for different systems and different time scales. To reproduce the results in the paper, one needs to obtain all 11 neural network models for each nonlinear system under study. For setup details, please refer to Table 2 in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import ResNet as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjustables\n",
    "k = list(range(11))            # model index: should be in {0, 2, ..., 10}\n",
    "dt = 0.01                    # time unit: 0.0005 for Lorenz and 0.01 for others\n",
    "system = 'Hyperbolic'         # system name: 'Hyperbolic', 'Cubic', 'VanDerPol', 'Hopf'\n",
    "noise = 0.0                    #noise levels: 0.0, 0.01, 0.02, 0.05 ,0.1, 0.2\n",
    "\n",
    "lr = 1e-3                     # learning rate\n",
    "max_epoch = 10000            # the maximum training epoch\n",
    "batch_size = 320              # training batch size\n",
    "arch = [2, 128, 128, 128, 2]  # architecture of the neural network (check paper for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = os.path.join('../../data/', system)\n",
    "model_dir = os.path.join('../../models/', system)\n",
    "\n",
    "# global const\n",
    "n_forward = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dt's: \n",
      "0.01\n",
      "0.02\n",
      "0.04\n",
      "0.08\n",
      "0.16\n",
      "0.32\n",
      "0.64\n",
      "1.28\n",
      "2.56\n",
      "5.12\n",
      "10.24\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_data = np.load(os.path.join(data_dir, 'train_noise{}.npy'.format(noise)))\n",
    "val_data = np.load(os.path.join(data_dir, 'val_noise{}.npy'.format(noise)))\n",
    "test_data = np.load(os.path.join(data_dir, 'test_noise{}.npy'.format(noise)))\n",
    "n_train = train_data.shape[0]\n",
    "n_val = val_data.shape[0]\n",
    "n_test = test_data.shape[0]\n",
    "\n",
    "# create dataset object\n",
    "datasets = list()\n",
    "step_sizes = list()\n",
    "step_size = [2**k for k in k]\n",
    "print('Dt\\'s: ')\n",
    "for i in range(11):\n",
    "    step_size = 2**i\n",
    "    print(step_size * dt)\n",
    "    step_sizes.append(step_size)\n",
    "    datasets.append(net.DataSet(train_data, val_data, test_data, dt, step_size=step_size, n_forward=n_forward))\n",
    "\n",
    "# dataset = net.DataSet(train_data, val_data, test_data, dt, step_size, n_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model_D1 ...\n",
      "epoch 1000, training loss 0.004603048786520958, validation loss 0.005144072230905294\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 0.004512108396738768, validation loss 0.0051805367693305016\n",
      "epoch 3000, training loss 0.004212073050439358, validation loss 0.005234193056821823\n",
      "epoch 4000, training loss 0.00443781353533268, validation loss 0.005252656061202288\n",
      "epoch 5000, training loss 0.004236592445522547, validation loss 0.005396964494138956\n",
      "epoch 6000, training loss 0.004090677015483379, validation loss 0.0054458430968225\n",
      "epoch 7000, training loss 0.004101904109120369, validation loss 0.005491490475833416\n",
      "epoch 8000, training loss 0.0038857031613588333, validation loss 0.005526662338525057\n",
      "epoch 9000, training loss 0.003916052635759115, validation loss 0.005612837616354227\n",
      "epoch 10000, training loss 0.003770360955968499, validation loss 0.005738550331443548\n",
      "training model_D2 ...\n",
      "epoch 1000, training loss 0.004377912729978561, validation loss 0.004970471374690533\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 0.004228480160236359, validation loss 0.004964437801390886\n",
      "(--> new model saved @ epoch 2000)\n",
      "epoch 3000, training loss 0.004332155454903841, validation loss 0.005002564750611782\n",
      "epoch 4000, training loss 0.004271304700523615, validation loss 0.005045800935477018\n",
      "epoch 5000, training loss 0.004021435510367155, validation loss 0.0051237051375210285\n",
      "epoch 6000, training loss 0.00395647156983614, validation loss 0.0051064747385680676\n",
      "epoch 7000, training loss 0.0038739198353141546, validation loss 0.0052389600314199924\n",
      "epoch 8000, training loss 0.00391340721398592, validation loss 0.005354222841560841\n",
      "epoch 9000, training loss 0.003660614136606455, validation loss 0.005317371804267168\n",
      "epoch 10000, training loss 0.003671300131827593, validation loss 0.005392307415604591\n",
      "training model_D4 ...\n",
      "epoch 1000, training loss 0.004020547028630972, validation loss 0.004814724437892437\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 0.004035749472677708, validation loss 0.004838116466999054\n",
      "epoch 3000, training loss 0.0041199130937457085, validation loss 0.0048541054129600525\n",
      "epoch 4000, training loss 0.004020874388515949, validation loss 0.004881641361862421\n",
      "epoch 5000, training loss 0.003924584481865168, validation loss 0.004988479427993298\n",
      "epoch 6000, training loss 0.003947075922042131, validation loss 0.005006162449717522\n",
      "epoch 7000, training loss 0.003595594549551606, validation loss 0.005001253914088011\n",
      "epoch 8000, training loss 0.003585876664146781, validation loss 0.0050804330967366695\n",
      "epoch 9000, training loss 0.0037585305981338024, validation loss 0.005120495334267616\n",
      "epoch 10000, training loss 0.0035563469864428043, validation loss 0.005159374792128801\n",
      "training model_D8 ...\n",
      "epoch 1000, training loss 0.0036858224775642157, validation loss 0.004346929490566254\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 0.0038496709894388914, validation loss 0.0044090100564062595\n",
      "epoch 3000, training loss 0.003800357226282358, validation loss 0.004404477309435606\n",
      "epoch 4000, training loss 0.003665388561785221, validation loss 0.004417970310896635\n",
      "epoch 5000, training loss 0.003746858099475503, validation loss 0.004467958118766546\n",
      "epoch 6000, training loss 0.003656949382275343, validation loss 0.004488483536988497\n",
      "epoch 7000, training loss 0.0035993943456560373, validation loss 0.004558435175567865\n",
      "epoch 8000, training loss 0.0036648036912083626, validation loss 0.0045281946659088135\n",
      "epoch 9000, training loss 0.003541111946105957, validation loss 0.004616158083081245\n",
      "epoch 10000, training loss 0.0036582883913069963, validation loss 0.004741623532027006\n",
      "training model_D16 ...\n",
      "epoch 1000, training loss 0.0037111369892954826, validation loss 0.004050983116030693\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 0.0034606338012963533, validation loss 0.0039914445951581\n",
      "(--> new model saved @ epoch 2000)\n",
      "epoch 3000, training loss 0.0037897294387221336, validation loss 0.004033816047012806\n",
      "epoch 4000, training loss 0.0034104257356375456, validation loss 0.004098021890968084\n",
      "epoch 5000, training loss 0.0036582534667104483, validation loss 0.004080068785697222\n",
      "epoch 6000, training loss 0.003454521531239152, validation loss 0.004147390369325876\n",
      "epoch 7000, training loss 0.0033599664457142353, validation loss 0.00418355455622077\n",
      "epoch 8000, training loss 0.0032161858398467302, validation loss 0.004260086454451084\n",
      "epoch 9000, training loss 0.003268021158874035, validation loss 0.00424378365278244\n",
      "epoch 10000, training loss 0.0032175867818295956, validation loss 0.004305366892367601\n",
      "training model_D32 ...\n",
      "epoch 1000, training loss 0.003701750887557864, validation loss 0.003938999958336353\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 0.0036360169760882854, validation loss 0.003953378181904554\n",
      "epoch 3000, training loss 0.0033713357988744974, validation loss 0.003987704403698444\n",
      "epoch 4000, training loss 0.0035332601983100176, validation loss 0.0038806756492704153\n",
      "(--> new model saved @ epoch 4000)\n",
      "epoch 5000, training loss 0.003649441059678793, validation loss 0.00412536459043622\n",
      "epoch 6000, training loss 0.0033838769886642694, validation loss 0.003900079755112529\n",
      "epoch 7000, training loss 0.0034437761642038822, validation loss 0.003942287992686033\n",
      "epoch 8000, training loss 0.0035126400180161, validation loss 0.00393623998388648\n",
      "epoch 9000, training loss 0.003428338561207056, validation loss 0.003938401583582163\n",
      "epoch 10000, training loss 0.0030827075242996216, validation loss 0.003947996534407139\n",
      "training model_D64 ...\n",
      "epoch 1000, training loss 0.0034777317196130753, validation loss 0.004014153964817524\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 0.0035460020881146193, validation loss 0.0038492537569254637\n",
      "(--> new model saved @ epoch 2000)\n",
      "epoch 3000, training loss 0.003541991813108325, validation loss 0.003812115639448166\n",
      "(--> new model saved @ epoch 3000)\n",
      "epoch 4000, training loss 0.0033311331644654274, validation loss 0.0038741438183933496\n",
      "epoch 5000, training loss 0.0032743760384619236, validation loss 0.0037904761265963316\n",
      "(--> new model saved @ epoch 5000)\n",
      "epoch 6000, training loss 0.0033071902580559254, validation loss 0.0037943359930068254\n",
      "epoch 7000, training loss 0.003237304277718067, validation loss 0.0038468404673039913\n",
      "epoch 8000, training loss 0.0030391907785087824, validation loss 0.0038725691847503185\n",
      "epoch 9000, training loss 0.0031203599646687508, validation loss 0.003827192122116685\n",
      "epoch 10000, training loss 0.0032238122075796127, validation loss 0.003904912620782852\n",
      "training model_D128 ...\n",
      "epoch 1000, training loss 0.0034244232811033726, validation loss 0.00369840394705534\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 0.003293492365628481, validation loss 0.0037587396800518036\n",
      "epoch 3000, training loss 0.0032250413205474615, validation loss 0.003773018717765808\n",
      "epoch 4000, training loss 0.003501737955957651, validation loss 0.0037933681160211563\n",
      "epoch 5000, training loss 0.003440829226747155, validation loss 0.0038062024395912886\n",
      "epoch 6000, training loss 0.0034117496106773615, validation loss 0.003770645475015044\n",
      "epoch 7000, training loss 0.003187228227034211, validation loss 0.003832767019048333\n",
      "epoch 8000, training loss 0.003184392349794507, validation loss 0.0038024899549782276\n",
      "epoch 9000, training loss 0.0033501971047371626, validation loss 0.003777666948735714\n",
      "epoch 10000, training loss 0.003021710319444537, validation loss 0.003838211065158248\n",
      "training model_D256 ...\n",
      "epoch 1000, training loss 0.005214112810790539, validation loss 0.003952912520617247\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 0.0034411170054227114, validation loss 0.003957656212151051\n",
      "epoch 3000, training loss 0.0030818290542811155, validation loss 0.003750812727957964\n",
      "(--> new model saved @ epoch 3000)\n",
      "epoch 4000, training loss 0.004022145178169012, validation loss 0.0037500502075999975\n",
      "(--> new model saved @ epoch 4000)\n",
      "epoch 5000, training loss 0.004361240193247795, validation loss 0.0037350282073020935\n",
      "(--> new model saved @ epoch 5000)\n",
      "epoch 6000, training loss 0.0037157353945076466, validation loss 0.0037297902163118124\n",
      "(--> new model saved @ epoch 6000)\n",
      "epoch 7000, training loss 0.0042705172672867775, validation loss 0.00393453985452652\n",
      "epoch 8000, training loss 0.0033267345279455185, validation loss 0.0038879539351910353\n",
      "epoch 9000, training loss 0.00317503628320992, validation loss 0.0037933397106826305\n",
      "epoch 10000, training loss 0.003423582063987851, validation loss 0.0037682673428207636\n",
      "training model_D512 ...\n",
      "epoch 1000, training loss 0.004775417037308216, validation loss 0.004013160243630409\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 0.004655755590647459, validation loss 0.003868548898026347\n",
      "(--> new model saved @ epoch 2000)\n",
      "epoch 3000, training loss 0.0032501965761184692, validation loss 0.003862868994474411\n",
      "(--> new model saved @ epoch 3000)\n",
      "epoch 4000, training loss 0.003827914595603943, validation loss 0.0038349132519215345\n",
      "(--> new model saved @ epoch 4000)\n",
      "epoch 5000, training loss 0.0040202937088906765, validation loss 0.0038237355183809996\n",
      "(--> new model saved @ epoch 5000)\n",
      "epoch 6000, training loss 0.004301257897168398, validation loss 0.003875408321619034\n",
      "epoch 7000, training loss 0.0037202744279056787, validation loss 0.0038241283036768436\n",
      "epoch 8000, training loss 0.0037092543207108974, validation loss 0.003939568065106869\n",
      "epoch 9000, training loss 0.00388245633803308, validation loss 0.0041820514015853405\n",
      "epoch 10000, training loss 0.003986735362559557, validation loss 0.004262426868081093\n",
      "training model_D1024 ...\n",
      "epoch 1000, training loss 0.0036745311226695776, validation loss 0.0041202642023563385\n",
      "(--> new model saved @ epoch 1000)\n",
      "epoch 2000, training loss 0.004088981077075005, validation loss 0.004294757265597582\n",
      "epoch 3000, training loss 0.0031924061477184296, validation loss 0.004166545812040567\n",
      "epoch 4000, training loss 0.0039107296615839005, validation loss 0.003945176023989916\n",
      "(--> new model saved @ epoch 4000)\n",
      "epoch 5000, training loss 0.003713954472914338, validation loss 0.004014593083411455\n",
      "epoch 6000, training loss 0.004048643168061972, validation loss 0.004346997942775488\n",
      "epoch 7000, training loss 0.00347590702585876, validation loss 0.004124501720070839\n",
      "epoch 8000, training loss 0.003383553819730878, validation loss 0.004030426498502493\n",
      "epoch 9000, training loss 0.003445267677307129, validation loss 0.004217163193970919\n",
      "epoch 10000, training loss 0.00337568880058825, validation loss 0.004115070682018995\n",
      "models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "models=list()\n",
    "for (step_size, dataset) in zip(step_sizes, datasets):\n",
    "    print('training model_D{} ...'.format(step_size))\n",
    "    model_name = 'model_D{}_noise{}.pt'.format(step_size, noise)\n",
    "    #model object\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    #model = torch.load(os.path.join(model_dir, model_name), map_location=device)\n",
    "    #model.device = device\n",
    "    # set up the network\n",
    "    model = net.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "    # training\n",
    "    model.train_net(dataset, max_epoch=max_epoch, batch_size=batch_size, lr=lr,  model_path=os.path.join(model_dir, model_name))\n",
    "    models.append(model)\n",
    "\n",
    "print('models trained successfully!')\n",
    "# create/load model object\n",
    "# try:\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     model = torch.load(os.path.join(model_dir, model_name), map_location=device)\n",
    "#     model.device = device\n",
    "# except:\n",
    "#     print('create model {} ...'.format(model_name))\n",
    "#     model = net.ResNet(arch=arch, dt=dt, step_size=step_size)\n",
    "#\n",
    "# # training\n",
    "# model.train_net(dataset, max_epoch=max_epoch, batch_size=batch_size, lr=lr,\n",
    "#                 model_path=os.path.join(model_dir, model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
